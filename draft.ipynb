{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"datasets\").exists():\n",
    "    os.mkdir(\"datasets\")\n",
    "if not Path(\"results\").exists():\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining limits on number of nodes\n",
    "n_min = 100\n",
    "n_max = 250\n",
    "\n",
    "# number of graphs\n",
    "num_g = 100\n",
    "\n",
    "# number of node features\n",
    "n_nf = 3\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "labels = []\n",
    "node_features = []\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 5\n",
    "m_max = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding 100 random graphs (label 0)\n",
    "for i in range(num_g):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "\n",
    "    g = nx.fast_gnp_random_graph(rand_n, rand_p)\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    graphs.append(g)\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding 100 powerlaw cluster graphs (label 1)\n",
    "for i in range(num_g):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "    rand_m = np.random.randint(m_min, m_max)\n",
    "\n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    graphs.append(g)\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding 100 watts strogatz graphs (label 2)\n",
    "for i in range(num_g):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "    rand_m = np.random.randint(m_min, m_max)\n",
    "\n",
    "    g = nx.watts_strogatz_graph(rand_n, rand_m, rand_p)\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    graphs.append(g)\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "    labels.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting this data into the format required for hcga\n",
    "\n",
    "from hcga.graph import Graph, GraphCollection\n",
    "\n",
    "# create graph collection object\n",
    "g_c = GraphCollection()\n",
    "\n",
    "# add graphs, node features and labels to the object\n",
    "g_c.add_graph_list(graphs, node_features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 300 graphs\n",
      "There are 3 features per node\n"
     ]
    }
   ],
   "source": [
    "# perform some sanity checks\n",
    "\n",
    "print(\"There are {} graphs\".format(len(g_c.graphs)))\n",
    "print(\"There are {} features per node\".format(g_c.get_n_node_features()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can save this if we want to and run everything from the command line\n",
    "from hcga.io import save_dataset\n",
    "\n",
    "save_dataset(\n",
    "    g_c,\n",
    "    \"labelled_graph_dataset\",\n",
    "    folder=\"./datasets/labelled_graph\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import hcga object\n",
    "from hcga.hcga import Hcga\n",
    "\n",
    "# define an object\n",
    "h = Hcga()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously saved dataset\n",
    "h.load_data(\n",
    "    \"./datasets/labelled_graph/labelled_graph_dataset.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hcga.extraction:Setting up feature classes...\n",
      " 88%|████████▊ | 38/43 [00:05<00:00,  7.59it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# extracting all features here\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfast\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# saving all features into a pickle\u001b[39;00m\n\u001b[1;32m      5\u001b[0m h\u001b[38;5;241m.\u001b[39msave_features(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results/labelled_graph/features.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/hcga.py:80\u001b[0m, in \u001b[0;36mHcga.extract\u001b[0;34m(self, n_workers, mode, norm, stats_level, runtimes, node_feat, timeout, connected, prediction_set)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Exctract features.\"\"\"\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prediction_set:\n\u001b[0;32m---> 80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_info \u001b[38;5;241m=\u001b[39m \u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatistics_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_runtimes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruntimes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_node_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnode_feat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconnected\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconnected\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m prediction_set:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_prediction_set, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures_info_prediction_set \u001b[38;5;241m=\u001b[39m extract(\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_graphs,\n\u001b[1;32m     94\u001b[0m         n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(n_workers),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m         connected\u001b[38;5;241m=\u001b[39mconnected,\n\u001b[1;32m    102\u001b[0m     )\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/extraction.py:63\u001b[0m, in \u001b[0;36mextract\u001b[0;34m(graphs, n_workers, mode, normalize_features, statistics_level, with_runtimes, with_node_features, timeout, connected, weighted)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m weighted:\n\u001b[1;32m     61\u001b[0m     graphs\u001b[38;5;241m.\u001b[39mremove_edge_weights()\n\u001b[0;32m---> 63\u001b[0m feat_classes, features_info_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_list_feature_classes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatistics_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatistics_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_node_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_node_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_runtimes:\n\u001b[1;32m     71\u001b[0m     L\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuntime option enable, we will only use 10 graphs and one worker to estimate \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124m        the computational time of each feature class.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m     )\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/extraction.py:168\u001b[0m, in \u001b[0;36mget_list_feature_classes\u001b[0;34m(mode, normalize_features, statistics_level, n_node_features, timeout)\u001b[0m\n\u001b[1;32m    166\u001b[0m list_feature_classes\u001b[38;5;241m.\u001b[39mappend(feature_class)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# runs once update_feature with trivial graph to create class variables\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m features_info \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatistics_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstatistics_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_node_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_node_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m columns \u001b[38;5;241m=\u001b[39m [(feature_class\u001b[38;5;241m.\u001b[39mshortname, col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m features_info\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m    175\u001b[0m feature_info_df[columns] \u001b[38;5;241m=\u001b[39m features_info\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/feature_class.py:156\u001b[0m, in \u001b[0;36mFeatureClass.setup_class\u001b[0;34m(cls, normalize_features, statistics_level, n_node_features, timeout)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mn_node_features \u001b[38;5;241m=\u001b[39m n_node_features\n\u001b[1;32m    155\u001b[0m inst \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(get_trivial_graph(n_node_features\u001b[38;5;241m=\u001b[39mn_node_features))\n\u001b[0;32m--> 156\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m feature_info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features:\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/feature_class.py:380\u001b[0m, in \u001b[0;36mFeatureClass.get_features\u001b[0;34m(self, all_features)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShortname not set for feature class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_features \u001b[38;5;241m=\u001b[39m all_features\n\u001b[0;32m--> 380\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_features:\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_normalize_features()\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/features/node_features.py:101\u001b[0m, in \u001b[0;36mNodeFeatures.compute_features\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_features\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode_feature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_feature_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mThe summary statistics of node feature \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mInterpretabilityScore\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstatistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnode_features\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_feature(\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv_node_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    111\u001b[0m         conv_node_feature,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m         statistics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_features\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_feature(\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv2_node_feature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m         conv2_node_feature,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m         statistics\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_features\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    123\u001b[0m     )\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/feature_class.py:353\u001b[0m, in \u001b[0;36mFeatureClass.add_feature\u001b[0;34m(self, feature_name, feature_function, feature_description, feature_interpret, function_args, statistics)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clustering_statistics(\n\u001b[1;32m    346\u001b[0m         func_result,\n\u001b[1;32m    347\u001b[0m         feature_name,\n\u001b[1;32m    348\u001b[0m         feature_description,\n\u001b[1;32m    349\u001b[0m         feature_interpret,\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m statistics \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnode_features\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 353\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node_feature_statistics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_interpret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m statistics \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_list_statistics(\n\u001b[1;32m    362\u001b[0m         func_result,\n\u001b[1;32m    363\u001b[0m         feature_name,\n\u001b[1;32m    364\u001b[0m         feature_description,\n\u001b[1;32m    365\u001b[0m         feature_interpret,\n\u001b[1;32m    366\u001b[0m     )\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/feature_class.py:431\u001b[0m, in \u001b[0;36mFeatureClass._node_feature_statistics\u001b[0;34m(self, feat_dist, feat_name, feat_desc, feat_interpret)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes summary statistics of each feature distribution.\"\"\"\u001b[39;00m\n\u001b[1;32m    430\u001b[0m feat_dist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(feat_dist)\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node_feats \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mfeat_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_statistics_basic(\n\u001b[1;32m    433\u001b[0m         feat_dist[:, node_feats],\n\u001b[1;32m    434\u001b[0m         feat_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(node_feats),\n\u001b[1;32m    435\u001b[0m         feat_desc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(node_feats),\n\u001b[1;32m    436\u001b[0m         feat_interpret,\n\u001b[1;32m    437\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# extracting all features here\n",
    "h.extract(mode=\"fast\", n_workers=4, timeout=5)\n",
    "\n",
    "# saving all features into a pickle\n",
    "h.save_features(\"./results/labelled_graph/features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved features\n",
    "\n",
    "h.load_features(\"./results/labelled_graph/features.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a classification analyse of the features\n",
    "\n",
    "h.analyse_features(\n",
    "    feature_file=\"./results/labelled_graph/features.pkl\",\n",
    "    results_folder=\"./results/labelled_graph\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_matrix, top_features = h.pairwise_classification(\n",
    "    feature_file=\"./results/labelled_graph/featuress.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(accuracy_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the top features for classifying between class 0 and class 1?\n",
    "print(top_features[(0.0, 1.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining limits on number of nodes\n",
    "n_min = 100\n",
    "n_max = 250\n",
    "\n",
    "# number of graphs\n",
    "num_g = 20\n",
    "\n",
    "# number of node features - in this example I will generate random node features that aren't useful for classifcation\n",
    "n_nf = 3\n",
    "\n",
    "\n",
    "# empty list of graphs and labels\n",
    "graphs = []\n",
    "node_features = []\n",
    "\n",
    "\n",
    "# setting limits on probability of edge existing for random graphs\n",
    "p_min = 0.1\n",
    "p_max = 0.5\n",
    "\n",
    "# adding 20 random graphs (label 0)\n",
    "for i in range(num_g):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "\n",
    "    g = nx.fast_gnp_random_graph(rand_n, rand_p)\n",
    "    g.label = 0\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    for i, node in enumerate(g.nodes):\n",
    "        g.nodes[node][\"features\"] = node_feat_matrix[i, :]\n",
    "\n",
    "    # graphs.append(g)\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g) * 2)\n",
    "\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "\n",
    "# setting limits on number of edges to add per node\n",
    "m_min = 5\n",
    "m_max = 10\n",
    "\n",
    "# adding 20 powerlaw cluster graphs (label 1)\n",
    "for i in range(num_g):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "    rand_m = np.random.randint(m_min, m_max)\n",
    "\n",
    "    g = nx.powerlaw_cluster_graph(rand_n, rand_m, rand_p)\n",
    "    g.label = 1\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    for i, node in enumerate(g.nodes):\n",
    "        g.nodes[node][\"features\"] = node_feat_matrix[i, :]\n",
    "\n",
    "    # graphs.append(g)\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g) * 2)\n",
    "    node_features.append(node_feat_matrix)\n",
    "\n",
    "\n",
    "# adding 20 watts strogatz graphs (label 2)\n",
    "for i in range(num_g):\n",
    "    rand_n = np.random.randint(n_min, n_max)\n",
    "    rand_p = np.random.randint(int(p_min * 100), int(p_max * 100)) / 100\n",
    "    rand_m = np.random.randint(m_min, m_max)\n",
    "\n",
    "    g = nx.watts_strogatz_graph(rand_n, rand_m, rand_p)\n",
    "    g.label = 2\n",
    "\n",
    "    node_feat_matrix = np.random.random((rand_n, n_nf))\n",
    "\n",
    "    for i, node in enumerate(g.nodes):\n",
    "        g.nodes[node][\"features\"] = node_feat_matrix[i, :]\n",
    "\n",
    "    # graphs.append(g)\n",
    "\n",
    "    graphs.append(nx.to_numpy_array(g) * 2)\n",
    "    node_features.append(node_feat_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 graphs in the unlabelled dataset\n",
      "There are 3 features per node\n"
     ]
    }
   ],
   "source": [
    "# create graph collection object\n",
    "graphs_unlabelled = GraphCollection()\n",
    "graphs_unlabelled.add_graph_list(graphs, node_features)  # loaded without the labels\n",
    "\n",
    "# save the unlabelled dataset\n",
    "save_dataset(\n",
    "    graphs_unlabelled, \"unlabelled_graph\", folder=\"./datasets\"\n",
    ")\n",
    "\n",
    "# perform some sanity checks\n",
    "print(\n",
    "    \"There are {} graphs in the unlabelled dataset\".format(\n",
    "        len(graphs_unlabelled.graphs)\n",
    "    )\n",
    ")\n",
    "print(\"There are {} features per node\".format(graphs_unlabelled.get_n_node_features()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './datasets/unlabelled_graph_dataset.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# extract features for the secondary dataset with no labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./datasets/unlabelled_graph_dataset.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m  \u001b[38;5;66;03m# set prediction graphs to True\u001b[39;00m\n\u001b[1;32m      5\u001b[0m h\u001b[38;5;241m.\u001b[39mextract(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfast\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)  \u001b[38;5;66;03m# set prediction set to True\u001b[39;00m\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/hcga.py:35\u001b[0m, in \u001b[0;36mHcga.load_data\u001b[0;34m(self, dataset, prediction_graphs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prediction_graphs:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# load graphs from pickle\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraphs \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;66;03m# take graphs as list input\u001b[39;00m\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# save_dataset(dataset, labels, dataset_name, folder=folder)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# self.graphs = load_dataset(dataset)\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/group_project/hcga/hcga/io.py:56\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(filename):\n\u001b[1;32m     55\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a dataset from a pickle.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './datasets/unlabelled_graph_dataset.pkl'"
     ]
    }
   ],
   "source": [
    "# extract features for the secondary dataset with no labels\n",
    "h.load_data(\n",
    "    \"./datasets/unlabelled_graph_dataset.pkl\"\n",
    ")  # set prediction graphs to True\n",
    "h.extract(mode=\"fast\", n_workers=4, timeout=20)  # set prediction set to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.analyse_features(\n",
    "    plot=False,\n",
    "    trained_model=\"./results/test/fitted_model\",\n",
    "    results_folder=\"./results/test\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"./results/test/prediction_results.csv\", index_col=0)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we use the default\n",
    "model = \"XG\"\n",
    "h.analyse_features(\n",
    "    model=model,\n",
    "    plot=False,\n",
    "    feature_file=\"./results/labelled_graph_dataset/features.pkl\",\n",
    "    results_folder=\"./results/labelled_graph_dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"RF\"\n",
    "h.analyse_features(\n",
    "    model=model,\n",
    "    plot=False,\n",
    "    feature_file=\"./results/labelled_graph_dataset/features.pkl\",\n",
    "    results_folder=\"./results/labelled_graph_dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(\n",
    "    probability=True\n",
    ")  # it is necessary to use probability=True to compute SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can compute with shap values\n",
    "h.analyse_features(\n",
    "    compute_shap=False,\n",
    "    model=model,\n",
    "    plot=False,\n",
    "    feature_file=\"./results/labelled_graph_dataset/features.pkl\",\n",
    "    results_folder=\"./results/labelled_graph_dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or with shap values:\n",
    "# WARNING the Kernel Explainer (for general models) is slow and requires a lot of memory\n",
    "h.analyse_features(\n",
    "    compute_shap=True,\n",
    "    kfold=False,\n",
    "    model=model,\n",
    "    plot=False,\n",
    "    feature_file=\"./results/labelled_graph_dataset/features.pkl\",\n",
    "    results_folder=\"./results/labelled_graph_dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "h.analyse_features(\n",
    "    compute_shap=False,\n",
    "    model=model,\n",
    "    plot=False,\n",
    "    feature_file=\"./results/labelled_graph_dataset/features.pkl\",\n",
    "    results_folder=\"./results/labelled_graph_dataset\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PoP_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
